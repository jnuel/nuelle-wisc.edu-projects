{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'log p(y=2020|x)': -3906.3519458841056,\n",
       " 'log p(y=2016|x)': -3916.458747858928,\n",
       " 'predicted y': '2020'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "\n",
    "def create_bow(vocab, filepath):\n",
    "    \"\"\" Create a single dictionary for the data\n",
    "        Note: label may be None\n",
    "    \"\"\"\n",
    "    bow = {}\n",
    "    # This function takes a path to a text file (assume a valid format, one token per line), reads the file in, \n",
    "    f = open(filepath, \"r\")\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip();\n",
    "#         print(line)\n",
    "        # creates a bag-of-words representation based on the vocabulary, \n",
    "        if line in vocab:\n",
    "            if line in bow:\n",
    "                bow.update({line: bow[line] + 1})\n",
    "#                 bow[line] += 1\n",
    "            else:\n",
    "                bow[line] = 1\n",
    "        elif None in bow:\n",
    "            # Give all counts of word types not in the vocabulary to out of vocabulary (OOV)\n",
    "#             bow[None] += 1\n",
    "            bow.update({None: bow[None] + 1})\n",
    "        else:\n",
    "            bow[None] = 1\n",
    "#         print(bow)\n",
    "    # and returns the bag-of-words in dictionary format.   \n",
    "    return bow\n",
    "\n",
    "def load_training_data(vocab, directory):\n",
    "    \"\"\" Create the list of dictionaries \"\"\"\n",
    "    dataset = []\n",
    "    # load the entire contents of the training directory into such Python dictionaries, \n",
    "    for folder in os.listdir(directory):\n",
    "#         print(folder)\n",
    "        if not folder.startswith('.'):\n",
    "            for filename in os.listdir(os.path.join(directory,folder)):\n",
    "#             print(filename)\n",
    "#             f = open(os.path.join(directory,folder,filename), \"r\")\n",
    "                bow = create_bow(vocab, os.path.join(directory,folder,filename))\n",
    "                pair = {}\n",
    "                # label them with their corresponding subdirectory label ('2016' or '2020' as strings) \n",
    "                pair['label'] = folder\n",
    "                pair['bow'] = bow\n",
    "                dataset.append(pair)\n",
    "    # return them in a list of length n=number of training documents.\n",
    "#     print(dataset)\n",
    "    return dataset\n",
    "\n",
    "def create_vocabulary(directory, cutoff):\n",
    "    \"\"\" Create a vocabulary from the training directory\n",
    "        return a sorted vocabulary list\n",
    "    \"\"\"\n",
    "\n",
    "    vocab = []\n",
    "    count_vocab = {}\n",
    "    \n",
    "    # TODO: add your code here\n",
    "    # To create the vocabulary for your classifier, \n",
    "    # traverse BOTH of these subdirectories under train/ (note: do not include test/) \n",
    "    for folder in os.listdir(directory):\n",
    "#         print(folder)\n",
    "        if not folder.startswith('.'): \n",
    "            for filename in os.listdir(os.path.join(directory,folder)):\n",
    "#             print(filename)\n",
    "\n",
    "                f = open(os.path.join(directory,folder,filename), \"r\")\n",
    "                lines = f.readlines()\n",
    "                # and count the number of times a word type appears in any file in either directory.\n",
    "                for line in lines:\n",
    "                    line = line.strip();\n",
    "                    if line not in count_vocab:\n",
    "                        count_vocab[line] = 1\n",
    "                    else:\n",
    "                        count_vocab[line] += 1\n",
    "    # As a design choice, we will exclude any word types which appear at a frequency strictly less than the cutoff argument \n",
    "    # (cutoff = 1 means retain all word types you encounter).\n",
    "#     print(count_vocab)  \n",
    "    for each_word in count_vocab:\n",
    "        if count_vocab[each_word] >= cutoff:\n",
    "            vocab.append(each_word)\n",
    "#     print(vocab)\n",
    "\n",
    "    # Return a sorted list of these word types.\n",
    "    vocab = sorted(vocab)\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "def prior(training_data, label_list):\n",
    "    \"\"\" return the prior probability of the label in the training set\n",
    "        => frequency of DOCUMENTS\n",
    "    \"\"\"\n",
    "    # This method should return the log probability- logP(label) of the labels in the training set. \n",
    "    # In order to calculate these, you will need to count the number of documents with each label in the training data, found in the training/ subdirectory.\n",
    "\n",
    "    smooth = 1 # smoothing factor\n",
    "    logprob = {}\n",
    "    num_label = {}\n",
    "    # TODO: add your code here\n",
    "    for each in training_data:\n",
    "        if each['label'] in num_label:\n",
    "            num_label[each['label']] += 1\n",
    "        else:\n",
    "            num_label[each['label']] = 1\n",
    "#     print(num_label)\n",
    "    total_files = len(training_data)\n",
    "    for each in label_list:\n",
    "        logprob[each] = np.log((num_label[each] + 1) / (total_files + 2))\n",
    "    return logprob\n",
    "\n",
    "def p_word_given_label(vocab, training_data, label):\n",
    "    \"\"\" return the class conditional probability of label over all words, with smoothing \"\"\"\n",
    "    \n",
    "    smooth = 1 # smoothing factor\n",
    "    word_prob = {}\n",
    "    \n",
    "#     print(vocab)\n",
    "#     print(training_data)\n",
    "    not_in_words = 0\n",
    "    total_words = 0\n",
    "    # For each vocab word wih the label\n",
    "    # For each vocab word, count how many times it appears in each BOW with the right label\n",
    "    for each_vocab in vocab:\n",
    "        word_prob[each_vocab] = 0\n",
    "#     print(word_prob)\n",
    "    for each_train in training_data:\n",
    "        if each_train['label'] == label:\n",
    "#             print(\"The bag of words\", each_train['bow'])\n",
    "            for each_word in each_train['bow']:\n",
    "#                 print(\"current word:\", each_word)\n",
    "                if each_word in vocab:\n",
    "                    word_prob[each_word] += each_train['bow'][each_word]\n",
    "                elif each_word == None:\n",
    "#                     print(\"in none\")\n",
    "                    if None in word_prob:\n",
    "                        word_prob[None] += each_train['bow'][each_word]\n",
    "                    else:\n",
    "                        word_prob[None] = each_train['bow'][each_word]\n",
    "                else:\n",
    "                    not_in_words += 1\n",
    "#     print(\"word prob\", word_prob)\n",
    "#     print(\"not in words\", not_in_words)\n",
    "    if None in word_prob:\n",
    "        word_prob[None] += not_in_words\n",
    "    else:\n",
    "        word_prob[None] = not_in_words\n",
    "        \n",
    "#     print(\"word prob\", word_prob)\n",
    "    \n",
    "    for each in word_prob:\n",
    "        total_words += word_prob[each]\n",
    "        \n",
    "#     print('total words(actual length of corpus):', total_words, 'lenvocab:', len(vocab))\n",
    "        \n",
    "    for each in word_prob:\n",
    "#         print('each', each, 'word_prob[each] + 1:',word_prob[each] + 1, '/ length of the corpus:', len(training_data),'len(vocab):',   len(vocab))\n",
    "        word_prob[each] = np.log((word_prob[each] + 1) / (total_words + len(vocab) + 1))\n",
    "\n",
    "    return word_prob\n",
    "\n",
    "    \n",
    "##################################################################################\n",
    "def train(training_directory, cutoff):\n",
    "    \"\"\" return a dictionary formatted as follows:\n",
    "            {\n",
    "             'vocabulary': <the training set vocabulary>,\n",
    "             'log prior': <the output of prior()>,\n",
    "             'log p(w|y=2016)': <the output of p_word_given_label() for 2016>,\n",
    "             'log p(w|y=2020)': <the output of p_word_given_label() for 2020>\n",
    "            }\n",
    "    \"\"\"\n",
    "    retval = {}\n",
    "    vocab = create_vocabulary(training_directory, cutoff)\n",
    "    retval['vocabulary'] = vocab\n",
    "    \n",
    "    training_data = load_training_data(vocab, training_directory)\n",
    "    the_prior = prior(training_data, ['2020', '2016'])\n",
    "    retval['log prior'] = the_prior\n",
    "    retval['log p(w|y=2020)'] = p_word_given_label(vocab, training_data, '2020')\n",
    "    retval['log p(w|y=2016)'] = p_word_given_label(vocab, training_data, '2016')\n",
    "\n",
    "    return retval\n",
    "\n",
    "\n",
    "def classify(model, filepath):\n",
    "    \"\"\" return a dictionary formatted as follows:\n",
    "            {\n",
    "             'predicted y': <'2016' or '2020'>, \n",
    "             'log p(y=2016|x)': <log probability of 2016 label for the document>, \n",
    "             'log p(y=2020|x)': <log probability of 2020 label for the document>\n",
    "            }\n",
    "    \"\"\"\n",
    "    retval = {}\n",
    "#     print(model)\n",
    "    log_prior_2016 = model['log prior']['2016']\n",
    "    log_prior_2020 = model['log prior']['2020']\n",
    "    total_2020_p = 0\n",
    "    total_2016_p = 0\n",
    "    bow = create_bow(model['vocabulary'], filepath)\n",
    "    \n",
    "    for each in bow:\n",
    "        total_2020_p += (model['log p(w|y=2020)'][each])*bow[each]\n",
    "        total_2016_p += (model['log p(w|y=2016)'][each])*bow[each]\n",
    "    \n",
    "    total_2020_p += log_prior_2020\n",
    "    total_2016_p += log_prior_2016\n",
    "    \n",
    "    retval['log p(y=2020|x)'] = total_2020_p\n",
    "    retval['log p(y=2016|x)'] = total_2016_p\n",
    "    if total_2020_p > total_2016_p:\n",
    "        retval['predicted y'] = '2020'\n",
    "    else:\n",
    "        retval['predicted y'] = '2016'\n",
    "\n",
    "    return retval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
