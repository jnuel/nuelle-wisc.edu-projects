Joy Nuelle , HW0 

When reading about the world of artificial intelligence and the bright prospects it can bring, one cannot help but consider the way that data and bias may play in AI, especially when research needs to be conducted for these complex algorithms. Stanford’s 100 Year Study on Artificial Intelligence makes several claims of what AI could bring to the future, and how it will be further integrated into everyday life. The article claims that, “But if employed with great care, greater reliance on AI may well result in a reduction in discrimination overall, since AI programs are inheritely more easily audited than humans” (Stanford, 36). The flaw in this argument is that the humans programming these AI machines, creating the algorithms and gathering data have unconscious bias so how can we expect AI to “overall reduce” discrimination? 
One of the arguments in the article of how AI can improve public safety and security is about predictive policing, and using AI to, “predict where and when crimes are more likely to happen and who may commit them” (37). While it then says that these algorithms have common misconceptions of targeting innocent people, certain data says otherwise. A very relevant and current problem that we are facing is the issue of Black people in America being targeted by police.  An article from the McKinsey Global Institute claims that, “In criminal justice models, oversampling certain neighborhoods because they are over policed can result in recording more crime, which results in more policing” (Silberg & Manyika). The data that is being fed into AI systems may already be coming from a biased perspective, where decisions on where and where not to police can largely affect crime rates in certain neighborhoods unfairly. Another article for the MIT Technology Review focused on why predictive policing programs are racist and claims that even though race isn’t allowed legally as a determining factor in the algorithms, “other variables, such as socioeconomic background, education, and zip code, act as proxies. Even without explicitly considering race, these tools are racist” (Heaven). The problem seems to lie in the data that researchers collect and the fact that humans are flawed leads developers to create flawed, biased algorithms. 
Another large challenge to developing AI that decreases the amount of discrimination in everyday life is that according to DataUSA, 76% of people who earn degrees in AI are male, and 44% of the people are white. The users of AI come from all different backgrounds, and without diverse representation in the field, there is little hope of creating algorithms that use minimal bias. The same McKinsey Global Institute article touches on the difficulty of defining the word fairness, and how humans may need to be held at a higher standard when making decisions on fairness.
Overall, the data that is collected for AI can actually contribute to discrimination, and “smart” algorithms must have bias and fairness at the forefront of consideration when in the process of creation. There will be hope as humans get better in being able to understand their own biases in everyday situations, and as the field of AI continues to become more diverse. However, claiming that AI will reduce discrimination due to feasibility of programming in contrast to humans is a circular argument that will only result in AI that reflects the creators and their biases as well.



References
Artificial Intelligence. (2019). Retrieved September 08, 2020, from https://datausa.io/profile/cip/artificial-intelligence

Heaven, Will Douglas. (2020, July 17). Predictive policing algorithms are racist. They need to be dismantled. MIT Technology Review. https://www.technologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice/

Silberg & Manyika. (2020, July 22). Tackling bias in artificial intelligence (and in humans). McKinsey Global Institute. https://www.mckinsey.com/featured-insights/artificial-intelligence/tackling-bias-in-artificial-intelligence-and-in-humans#


